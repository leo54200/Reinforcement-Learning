# CS672: Introduction to Reinforcement Learning - Core Algorithm Implementations

A comprehensive implementation of foundational and modern Reinforcement Learning (RL) algorithms. This repository covers the spectrum from exact Dynamic Programming to Model-Free control and Policy Gradient methods. Developed during the graduate course at **KAIST (Fall 2025)** under **Prof. Sungjin Ahn**.

---

## ðŸš€ Frameworks & Core Logic
### Homework 1: Dynamic Programming & MDPs
Solves known Markov Decision Processes (MDPs) by leveraging the environment's transition and reward dynamics.

* **Policy Iteration:** Finds the optimal policy through alternating cycles of **Policy Evaluation** (computing the value function for the current policy) and **Policy Improvement** (making the policy greedy with respect to the action-values).
* **Value Iteration:** Directly computes the optimal value function by iteratively applying the **Bellman Optimality Backup**, bypassing the need to wait for full policy convergence at each step.

### Homework 2: Model-Free Prediction & Control
Implements methods that learn directly from experience samples (episodes) without knowing the underlying transition probabilities or reward functions.

* **Monte-Carlo (MC) vs. TD(0):** Compares learning from full returns (MC) against **bootstrapping** from immediate rewards and estimated future values (Temporal Difference).
* **Sarsa (On-Policy):** An on-policy control algorithm that updates action-values based on the actual trajectory the agent follows, using the next action selected by the current policy.
* **Q-Learning (Off-Policy):** A landmark off-policy algorithm that learns the optimal action-value function by assuming a greedy target policy, independent of the agent's actual behavior.
* **Importance Sampling:** Corrects distribution mismatch by using an **Importance Sampling Ratio** to evaluate a target policy using data generated by a different behavior policy.

### Homework 3: Policy Gradient Methods
Explores "Policy-Based" RL, optimizing a parameterized policy directly via gradient ascent on the expected return.

* **REINFORCE:** A Monte-Carlo approach that utilizes the **Policy Gradient Theorem** to increase the probability of actions proportional to the total return they produce. 
* **Baseline Subtraction:** Addresses the high variance inherent in vanilla REINFORCE by subtracting a state-dependent baseline (the state-value function).
* **Advantage Function:** Focuses updates on the **Advantage**, reinforcing actions that perform significantly better than the average expectation for a given state.
---

## ðŸ“š Acknowledgements & Academic Integrity

Implemented by **leo54200** for the **CS672: Introduction to Reinforcement Learning** course at KAIST.
