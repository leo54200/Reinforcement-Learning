{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7InTcI1yCSk"
      },
      "source": [
        "# CS377 Reinforcement Learning - Programming Assignment 1\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submission Guideline"
      ],
      "metadata": {
        "id": "Rl4EvnTN8fQE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How to submit**\n",
        "*   Fill out <mark> TODO</mark> blocks, **DO NOT** modify other parts of the skeleton code.\n",
        "*   Submit one file: {student_ID}.ipynb to KLMS\n",
        "\n",
        "    e.g. 20251234.ipynb\n",
        "* **Late submission policy**: After the submission deadline, you will immediately lose 10% of the score, another 10% after 24 hours later, and so on. Submissions after 72 hours (3 days) will not be counted.\n",
        "\n",
        "**Note**\n",
        "*   Make a copy of this .ipynb file. Do not directly edit this file.\n",
        "*   You are required to use numpy, do not use neither pytorch nor tensorflow.\n",
        "*   Check whether your whole cells work well by restarting runtime code and running all before the submission.\n",
        "*   TA will look into the implemented functions, their validity and give corresponding score to each <mark> TODO</mark> problem.\n",
        "*   Ask questions through KLMS so that you can share information with other students.\n",
        "*   TA in charge: Junyeob Baek (wnsdlqjtm@kaist.ac.kr), Hosung Lee (hslee@kaist.ac.kr)"
      ],
      "metadata": {
        "id": "YMmWjluP8izk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In this programming assignment, you will**\n",
        "* Learn how to use `gym` environment for reinforcement learning framework\n",
        "* Implement basic dynamic programming algorithms"
      ],
      "metadata": {
        "id": "TfKWALsULr9E"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHkyvNOhp-zM"
      },
      "source": [
        "## 1. Gym Tutorial"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1. What is Gym?\n",
        "Gym is a project that provides an API for reinforcement learning environments that include implementations of common environments: cartpole, pendulum, mountain-car, mujoco, atari, and more. The details of gym package can be checked [here](https://www.gymlibrary.dev/content/basic_usage/).\n",
        "\n",
        "The API contains four key functions: `make`, `reset`, `step` and `render` that this basic usage will introduce you to. At the core of Gym is `Env` which is a high level python class representing a markov decision process from reinforcement learning theory."
      ],
      "metadata": {
        "id": "l1lNRBBta-I7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2. Initializing and Interacting with Environments\n",
        "The following is the basic preparation for installing gym and importing required packages."
      ],
      "metadata": {
        "id": "Jrk5tjotbzNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import time"
      ],
      "metadata": {
        "id": "iaxipovDwRd6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4b4f535-8a9f-42de-f812-5b57268349b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.12/dist-packages (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initializing environments is very easy in Gym and can be done via the `make` function. Throughout this homework, we will use `FrozenLake` environment."
      ],
      "metadata": {
        "id": "xzHd1rqRdpY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "map_size = 4\n",
        "is_slippery = False\n",
        "env = gym.make(\"FrozenLake-v1\", desc=None, map_name=f\"{map_size}x{map_size}\", is_slippery=is_slippery).unwrapped"
      ],
      "metadata": {
        "id": "Evpcctmzdyy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The classic **agent-environment loop** pictured below is simplified representation of reinforcement learning that Gym implements.\n",
        "\n",
        "<img src=\"https://gymnasium.farama.org/_images/AE_loop_dark.png\" width=\"340\" height=\"260\">\n",
        "\n",
        "This loop is implemented using the following gym code."
      ],
      "metadata": {
        "id": "Yp6twdk_eIkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "observation = env.reset()  # start new episode\n",
        "\n",
        "for _ in range(10):\n",
        "\n",
        "  action = env.action_space.sample()  # sample random action from env\n",
        "  print(action)\n",
        "  observation, reward, terminated, truncated, info = env.step(action)  # next state\n",
        "  print(observation, reward, terminated, info)\n",
        "\n",
        "  if terminated:  # if the episode is over\n",
        "    observation = env.reset()\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "id": "yDKqcq13fxPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After initializing the environment, we `reset` the environment to get the first observation of the environment.\n",
        "\n",
        "Next, the agent performs an action in the environment, `step`, this can be imagined as moving a agent or pressing a button on a gamesâ€™ controller that causes a change within the environment. As a result, the agent receives a new observation from the updated environment along with a reward for taking the action. This reward could be for instance positive for reaching to goal or a negative reward for moving into lava. One such action-observation exchange is referred to as a timestep.\n",
        "\n",
        "However, after some timesteps, the environment may end, this is called the terminal state. For instance, the robot may have crashed, or the agent have succeeded in completing a task, the environment will need to stop as the agent cannot continue. In gym, if the environment has terminated, this is returned by `step`. If terminated is true then reset should be called next to restart the environment."
      ],
      "metadata": {
        "id": "n_vBKhhqnXtG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3. FrozenLake Investigation"
      ],
      "metadata": {
        "id": "fbUGEyZubNXS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Throughout this HW, we are going to use `FronzenLake` environment. The goal of the agent in `FrozenLake` is **to navigate from the starting state (S) to the goal state (G) by walking only on frozen tiles (F) and avoid holes (H)**.\n",
        "\n",
        "We can have two sizes of environment:\n",
        "- `map_name=\"4x4\"`: a 4x4 grid version\n",
        "- `map_name=\"8x8\"`: a 8x8 grid version\n",
        "\n",
        "\n",
        "The environment has two modes:\n",
        "- `is_slippery=False`: The agent always move in the intended direction due to the non-slippery nature of the frozen lake.\n",
        "- `is_slippery=True`: The agent may not always move in the intended direction due to the slippery nature of the frozen lake (stochastic).\n",
        "\n",
        "The below is the visualization of `FrozenLake` environment. You can find more details of environment [here](https://www.gymlibrary.dev/environments/toy_text/frozen_lake/).\n",
        "\n",
        "![alt text](https://gymnasium.farama.org/_images/frozen_lake.gif)"
      ],
      "metadata": {
        "id": "R81xLQL9Yd_S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's check environment variables in `FrozenLake`."
      ],
      "metadata": {
        "id": "5U7hOD4dbkA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset()  # start a new episode\n",
        "\n",
        "obs_sample = env.observation_space.sample()\n",
        "act_sample = env.action_space.sample()\n",
        "\n",
        "print(\"Observation Space: \", env.observation_space)\n",
        "print(\"Observation Sample: \", obs_sample)  # Sample a random observation\n",
        "print(\"Action Space Dimension: \", env.action_space.n)\n",
        "print(\"Action Sample: \", act_sample)  # Sample a random action\n",
        "tr = env.P[obs_sample][act_sample]  # Transition probability\n",
        "print(\"Transition:\", tr)  # prob, next_state, reward, done"
      ],
      "metadata": {
        "id": "tS_z6UM3zyLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see with `Observation Space Shape Discrete(16)` that the observation is a value representing the **agentâ€™s current position as current_row * nrows + current_col (where both the row and col start at 0)**.\n",
        "\n",
        "For example, the goal position in the 4x4 map can be calculated as follows: 3 * 4 + 3 = 15. The number of possible observations is dependent on the size of the map. **For example, the 4x4 map has 16 possible observations.**\n",
        "\n",
        "The action space (the set of possible actions the agent can take) is discrete with 4 actions available ðŸŽ®:\n",
        "- 0: GO LEFT\n",
        "- 1: GO DOWN\n",
        "- 2: GO RIGHT\n",
        "- 3: GO UP\n",
        "\n",
        "Reward function ðŸ’°:\n",
        "- Reach goal: +1\n",
        "- Reach hole: 0\n",
        "- Reach frozen: 0\n",
        "\n",
        "`env.P[o][a]` gives list of transitions that can happen when doing action `a` at state `o`. Note that next state may not be deterministic (This will be important when we deal with `slippery FronzenLake`.  "
      ],
      "metadata": {
        "id": "vhY6tWljcWJ4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgUiFyZBp-zS"
      },
      "source": [
        "## 2. Dynamic Programming"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1. Policy Evaluation"
      ],
      "metadata": {
        "id": "2T8EILG5tKbx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's start implementing basic algorithms in Dynamic Programming! The first algorithm is ***policy evaluation***. If the environment's dynamics is completely known, we can calculate the value function using Bellman equation. Bellman equation for policy evaluation is as follows:\n",
        "$$v_{k+1}(s)=E_{\\pi}[R_{t+1}+\\gamma v_{k}(S_{t+1})|S_t=s] \\\\\n",
        "=\\sum_{a} \\pi(a|s) \\sum_{s',r} p(s',r|s,a) [r+\\gamma v_{k}(s')]\n",
        "$$\n",
        "For more detailed algorithm, refer to figure 4.1 of textbook."
      ],
      "metadata": {
        "id": "2pnOPzk_tnv_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will start from the random policy for policy evaluation and improve it later. First, let's implement policy that selects actions following uniform distribution.\n",
        "\n",
        "<mark>TODO-1</mark> Implement the random policy (discrete uniform)."
      ],
      "metadata": {
        "id": "JT9qT5dpgxkU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######## TODO ########\n",
        "# (hint) policy: numpy array with size [s,a], each element represents probability of action given state\n",
        "s = env.observation_space.n\n",
        "a = env.action_space.n\n",
        "random_policy = np.ones((s, a))/a\n",
        "print(random_policy.shape)\n",
        "print(random_policy)\n",
        "######################"
      ],
      "metadata": {
        "id": "UY5HIb6ShcOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To monitor how each DP algorithms run for each iterations, we want to visualize the value function with heatmap so that we can easily see how value changes when the policy changes (or update is done).\n",
        "\n",
        "<mark>TODO-2</mark> Implement the visualization of value table with heatmap."
      ],
      "metadata": {
        "id": "hR8d1DcOlKCH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_value_table(value, map_size):\n",
        "    \"\"\"\n",
        "    Visualze value function for FrozenLake environment\n",
        "\n",
        "    Args:\n",
        "        value: numpy array with size [s,], value table\n",
        "        map_size: int, FrozenLake map size\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    ######## TODO ########\n",
        "    # (hint) reshape value and use plt.matshow()\n",
        "    value_reshaped = value.reshape(map_size, map_size)\n",
        "    plt.matshow(value_reshaped, cmap=\"coolwarm\")\n",
        "    ######################\n",
        "\n",
        "    plt.clim(0, 1)\n",
        "    plt.colorbar()\n",
        "    plt.show()\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "Cb8eeR04l1JV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<mark>TODO-3</mark> Implement the policy evaluation function."
      ],
      "metadata": {
        "id": "lqjYmEDzg-nu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_evaluation(policy, env, discount_factor=0.9, theta=0.00001, map_size=4):\n",
        "    \"\"\"\n",
        "    Evaluate a policy given an environment and a full description of the environment's dynamics.\n",
        "\n",
        "    Args:\n",
        "        policy: numpy array with size [s,a], each element represents probability of action given state\n",
        "        env: gym environment\n",
        "        discount_factor: float, gamma discount factor\n",
        "        theta: float, stopping criteria\n",
        "        map_size: int, FrozenLake map size\n",
        "    Returns:\n",
        "        V_new: numpy array with size [s,]\n",
        "    \"\"\"\n",
        "    V_pre, V_new = None, None\n",
        "    ######## TODO ########\n",
        "    V_new = np.zeros(env.observation_space.n)  # start with zeros\n",
        "    while True:\n",
        "      V_pre = V_new.copy()\n",
        "      for state in range(env.observation_space.n):\n",
        "        v = 0\n",
        "        for action in range(env.action_space.n):\n",
        "          for prob, next_state, reward, done in env.P[state][action]:\n",
        "            v += policy[state][action] * prob * (reward + discount_factor * V_pre[next_state])\n",
        "        V_new[state] = v\n",
        "      if np.max(np.abs(V_new - V_pre)) < theta:\n",
        "        break\n",
        "\n",
        "    ######################\n",
        "    return V_new"
      ],
      "metadata": {
        "id": "pFdPpyi-g8vA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check value table\n",
        "value_table = policy_evaluation(random_policy, env)\n",
        "value_table_ans = np.array([0.0044593, 0.0042131, 0.01006041, 0.00411379,\n",
        "                            0.0067141, 0., 0.02633197, 0.,\n",
        "                            0.01867277, 0.05760567, 0.10697105, 0.,\n",
        "                            0., 0.13038228, 0.39148958, 0.])\n",
        "assert np.all(np.abs(value_table - value_table_ans) < 1e-3)\n",
        "visualize_value_table(value_table, map_size)"
      ],
      "metadata": {
        "id": "vre5DIypjP9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2. Policy Iteration"
      ],
      "metadata": {
        "id": "Kaan5GLntMkN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The reason for computing the value function for a policy is to help find better\n",
        "policies. Suppose we have determined the value function $v_{\\pi}$ for an arbitrary deterministic policy $\\pi$. Then, we can have new greedy policy $\\pi'$ via policy improvement from following equation.\n",
        "\n",
        "$$\n",
        "\\pi'(s)= \\text{argmax}_{a} \\sum_{s',r} p(s',r|s,a) [r+\\gamma v_{\\pi}(s')]\n",
        "$$\n",
        "\n",
        "Once a policy $\\pi$ has been improved using $v_{\\pi}$ to yield a better policy $\\pi'$, we can then compute $v_{\\pi'}$ and improve it again to yield an even better $\\pi''$. We can thus obtain a sequence of monotonically improving policies and value functions:\n",
        "\n",
        "$$\n",
        "\\pi_{0} âŸ¶^{e} v_{\\pi_{0}} âŸ¶^{i} \\pi_{1} âŸ¶^{e} v_{\\pi_{1}} âŸ¶^{i} \\pi_{2} â€¦ âŸ¶^{i} \\pi_{*} âŸ¶^{e} v_{\\pi_{*}}\n",
        "$$\n",
        "\n",
        "where $âŸ¶^{e}$ denotes policy evaluation and $âŸ¶^{i}$ denotes policy improvement. Each policy is guaranteed to be a strict improvement over the previous one (unless it is already optimal). Because a finite MDP has only a finite number\n",
        "of policies, this process must converge to an optimal policy and optimal value\n",
        "function in a finite number of iterations.\n",
        "\n",
        "This way of finding an optimal policy is called ***policy iteration***. For more detailed algorithm, refer to figure 4.3 of textbook."
      ],
      "metadata": {
        "id": "H_drugOCNxY5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For step-by-step implementation of the algorithm, let's implement a helper function first.\n",
        "One step lookahead function calculates action-value as follows.\n",
        "\n",
        "$$q_{\\pi}(s,a)= \\sum_{s',r} p(s',r|s,a) [r+\\gamma v_{\\pi}(s')]\n",
        "$$"
      ],
      "metadata": {
        "id": "sQVDAcscD1KK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<mark>TODO-4</mark> Implement the one step lookahead function."
      ],
      "metadata": {
        "id": "HF64lb7mEFCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def one_step_lookahead(state, V, env, discount_factor):\n",
        "    \"\"\"\n",
        "    Helper function to calculate the value for every action in a given state.\n",
        "\n",
        "    Args:\n",
        "        state: integer, current state\n",
        "        V: numpy array with size [s,], value table\n",
        "        env: gym environment\n",
        "        discount_factor: float, gamma discount factor\n",
        "    Returns:\n",
        "        A: numpy array with size [a,], action value table for state\n",
        "    \"\"\"\n",
        "    A = None\n",
        "    ######## TODO ########\n",
        "    A = np.zeros(env.action_space.n)\n",
        "    for action in range(env.action_space.n):\n",
        "      for prob, next_state, reward, done in env.P[state][action]:\n",
        "        A[action] += prob * (reward + discount_factor * V[next_state])\n",
        "    ######################\n",
        "    return A"
      ],
      "metadata": {
        "id": "azE0kOS7EnSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This time, we will start from **Going Left** policy (initial policy) for policy iteration.\n",
        "\n",
        "<mark>TODO-5</mark> Implement the **Going Left** policy."
      ],
      "metadata": {
        "id": "NfZ83h0VFSOC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def go_left_policy(env):\n",
        "    \"\"\"\n",
        "    Go Left Policy\n",
        "\n",
        "    Args:\n",
        "        env: gym environment\n",
        "    Returns:\n",
        "        policy: numpy array with size [s,a], each element represents probability of action given state\n",
        "    \"\"\"\n",
        "    policy = None\n",
        "    ######## TODO ########\n",
        "    policy = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "    policy[:, 0] = 1.0\n",
        "    ######################\n",
        "    return policy"
      ],
      "metadata": {
        "id": "2wcCqQqDFn_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To monitor how policy changes for each iterations, we want to visualize the policy in `FrozenLake` so that we can easily see how policy changes based on value function.\n",
        "\n",
        "<mark>TODO-6</mark> Implement the visualization of greedy policy."
      ],
      "metadata": {
        "id": "fsCpSPQxoKoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_greedy_policy(policy, map_size):\n",
        "    \"\"\"\n",
        "    Visualize greedy policy based on policy. For e.g., printed result will be\n",
        "\n",
        "    Reshaped Grid Policy (0=left, 1=down, 2=right, 3=up):\n",
        "    [[0 0 0 0]\n",
        "    [0 0 1 0]\n",
        "    [0 1 1 0]\n",
        "    [0 2 2 0]]\n",
        "\n",
        "    Args:\n",
        "        policy: numpy array with size [s,a], each element represents probability of action given state\n",
        "        map_size: int, FrozenLake map size\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    print(\"Reshaped Grid Policy (0=left, 1=down, 2=right, 3=up):\")\n",
        "    ######## TODO ########\n",
        "    policy_reshaped = np.argmax(policy, axis=1).reshape(map_size, map_size)\n",
        "    print(policy_reshaped)\n",
        "    ######################\n",
        "    return"
      ],
      "metadata": {
        "id": "EfWBeyJ4ofDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<mark>TODO-7</mark> Implement the policy iteration."
      ],
      "metadata": {
        "id": "GxGga42kUNfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_iteration(env, init_policy, discount_factor=0.9, map_size=4):\n",
        "    \"\"\"\n",
        "    Policy Improvement Algorithm. Iteratively evaluates and improves a policy until an optimal policy is found.\n",
        "\n",
        "    Args:\n",
        "        env: gym environment\n",
        "        init_policy: numpy array with size [s,a], each element represents probability of action given state\n",
        "        discount_factor: float, gamma discount factor\n",
        "        map_size: int, FrozenLake map size\n",
        "    Returns:\n",
        "        tuple of (policy, V)\n",
        "        policy: numpy array with size [s,a], each element represents probability of action given state\n",
        "        V: numpy array with size [s,], value table\n",
        "\n",
        "    (IMPORTANT!): For each iteration, use visualize_value_table, and visualize_greedy_policy function for visualization!\n",
        "    \"\"\"\n",
        "\n",
        "    policy, V = None, None\n",
        "    ######## TODO ########\n",
        "    policy = init_policy.copy()\n",
        "\n",
        "    while True:\n",
        "      V = policy_evaluation(policy, env, discount_factor)\n",
        "\n",
        "      policy_changed = False\n",
        "      new_policy = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "\n",
        "      for state in range(env.observation_space.n):\n",
        "        action_values = one_step_lookahead(state, V, env, discount_factor)\n",
        "\n",
        "        best_action = np.argmax(action_values)\n",
        "        new_policy[state, best_action] = 1.0\n",
        "        # check if policy changed\n",
        "        if not np.array_equal(new_policy[state], policy[state]):\n",
        "          policy_changed = True\n",
        "      visualize_value_table(V, map_size)\n",
        "      visualize_greedy_policy(new_policy, map_size)\n",
        "      if not policy_changed:\n",
        "        break\n",
        "      policy = new_policy\n",
        "    ######################\n",
        "\n",
        "    return policy, V"
      ],
      "metadata": {
        "id": "Dl2_PKPWr-NV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "policy, v = policy_iteration(env, go_left_policy(env))"
      ],
      "metadata": {
        "id": "ht82Uru5tsBh",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3. Value Iteration"
      ],
      "metadata": {
        "id": "hG8nYJRAxNi4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One drawback to policy iteration is that each of its iterations involves policy\n",
        "evaluation, which may itself be a protracted iterative computation requiring\n",
        "multiple sweeps through the state set.\n",
        "If policy evaluation is done iteratively, then convergence exactly to $v_{\\pi}$ occurs only in the limit. In fact, the policy evaluation step of policy iteration can be truncated in several ways without losing the convergence guarantees of policy iteration.\n",
        "One important special case is when policy evaluation is stopped after just one\n",
        "sweep (one backup of each state).\n",
        "This algorithm is called ***value iteration***.\n",
        "It can be written as a particularly simple backup operation that combines the policy improvement and truncated policy evaluation steps:\n",
        "\n",
        "$$\n",
        "v_{k+1}(s)=\\text{max}_{a} \\textbf{E}[R_{t+1}+\\gamma v_{k}(s_{t+1}) | S_t=s, A_t=a] \\\\\n",
        "= \\text{max}_{a} \\sum_{s',r} p(s',r|s,a) [r+\\gamma v_{k}(s')]\n",
        "$$\n",
        "\n",
        "for all $s \\in S$. For arbitrary $v_0$, the sequence $v_k$ can be shown to converge to $v_{*}$ under the same conditions that guarantee the existence of $v_{*}$.\n",
        "Finally, let us consider how value iteration terminates.\n",
        "Like policy evaluation, value iteration formally requires an infinite number of iterations to converge exactly to $v_{*}$.\n",
        "In practice, we stop once the value function changes by only a small amount in a sweep. For more detailed algorithm, refer to figure 4.5 of textbook.\n",
        "\n",
        "**Note that above equation holds when the policy is deterministic.**\n"
      ],
      "metadata": {
        "id": "ebEeOZHFgz2e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<mark>TODO-8</mark> Implement the value iteration."
      ],
      "metadata": {
        "id": "FMboH8B-ntXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def value_iteration(env, theta=0.0001, discount_factor=0.9, map_size=4):\n",
        "    \"\"\"\n",
        "    Value Iteration Algorithm.\n",
        "\n",
        "    Args:\n",
        "        env: gym environment\n",
        "        theta: float, stopping criteria\n",
        "        discount_factor: float, gamma discount factor\n",
        "        map_size: int, FrozenLake map size\n",
        "    Returns:\n",
        "        tuple of (policy, V)\n",
        "        policy: numpy array with size [s,a], each element represents probability of action given state\n",
        "        V: numpy array with size [s,], value table\n",
        "\n",
        "    (IMPORTANT!): For each iteration, use visualize_value_table for visualization!\n",
        "    \"\"\"\n",
        "    policy, V = None, None\n",
        "    ######## TODO ########\n",
        "    # 1. Initialization\n",
        "    V = np.zeros(env.observation_space.n)\n",
        "    iterations = 0\n",
        "    # 2. Value Iteration\n",
        "    while True:\n",
        "      V_old = V.copy()\n",
        "      iterations += 1\n",
        "      for state in range(env.observation_space.n):\n",
        "        action_values = one_step_lookahead(state, V_old, env, discount_factor)\n",
        "\n",
        "        V[state] = np.max(action_values)\n",
        "      delta = np.max(np.abs(V - V_old))\n",
        "      visualize_value_table(V, map_size)\n",
        "      if delta < theta:\n",
        "        break\n",
        "    # 3. Policy Extraction\n",
        "    policy = np.zeros([env.observation_space.n, env.action_space.n])\n",
        "    for state in range(env.observation_space.n):\n",
        "      action_values = one_step_lookahead(state, V, env, discount_factor)\n",
        "      best_action = np.argmax(action_values)\n",
        "      policy[state, best_action] = 1.0\n",
        "    print(\"Number of iterations:\", iterations)\n",
        "    ######################\n",
        "    return policy, V"
      ],
      "metadata": {
        "id": "RSLGPJIl_X2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "policy, v = value_iteration(env, discount_factor=0.9)\n",
        "visualize_greedy_policy(policy, map_size)"
      ],
      "metadata": {
        "id": "TlfKnQzF_0Vh",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<mark>TODO-9</mark> Answer the following questions.\n",
        "\n",
        "*   Try running value_iteration with different `discount_factor` (e.g., 0.1, 0.5), Is there any difference in result when $0<Î³<1$?\n",
        "*   Try running value_iteration with $Î³=1$. Explain the difference."
      ],
      "metadata": {
        "id": "ECiRBDpRbua0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<mark>Answer for TODO-9</mark>\n",
        "* Yes, varying the discount factor Î³ between 0 and 1 significantly changes the resulting optimal policy. The policies I found are different because Î³ determines the agent's planning horizon.\n",
        "Low Î³ (e.g., 0.1) creates a short-sighted agent. The value of the distant goal reward is heavily discounted, so the reward signal is too weak for the agent to form a coherent long-term plan. This results in a not so good policy.\n",
        "High Î³ (e.g., 0.9) creates a farsighted agent that values future rewards. The reward signal propagates effectively across the state space, allowing the algorithm to identify the globally optimal path to the goal.\n",
        "\n",
        "* Setting the discount factor Î³ = 1 means there is no discounting of future rewards. This fundamentally alters the optimization objective.\n",
        "The primary difference is that the algorithm is no longer incentivized to find the shortest path.\n",
        "With Î³ < 1, a shorter path is mathematically better because it results in a higher discounted reward. With Î³ = 1, any safe path that reaches the goal is considered equally optimal, as the undiscounted return is the same regardless of the number of steps taken."
      ],
      "metadata": {
        "id": "ZZmiS7pqcbP-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4. Asynchronous Dynamic Programming (In-place Value Iteration)"
      ],
      "metadata": {
        "id": "etOTBjFDezsm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A major drawback to the DP methods that we have discussed so far is that\n",
        "they involve operations over the entire state set of the MDP, that is, they\n",
        "require sweeps of the state set.\n",
        "If the state set is very large, then even a single sweep can be prohibitively expensive.\n",
        "For example, the game of Go has about $10^{172}$.\n",
        "Even if we could perform the value iteration backup on a million states per second, it would take forever to complete a single sweep.\n",
        "\n",
        "Asynchronous DP algorithms are in-place iterative DP algorithms that are\n",
        "not organized in terms of systematic sweeps of the state set.\n",
        "These algorithms back up the values of states in any order whatsoever, using whatever values of other states happen to be available.\n",
        "\n",
        "In this section, we will implement a simple asynchronous DP algorithm, which is in-place value iteration. This version of asynchronous value iteration backs up the value, in place, of only one state, $s_k$, on each step, $k$, using the value iteration backup."
      ],
      "metadata": {
        "id": "-x1KFYkNKG1A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<mark>TODO-10</mark> Implement the asynchronous value iteration."
      ],
      "metadata": {
        "id": "avidsVpO_0b5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def asynchronous_value_iteration(env, theta=0.0001, discount_factor=0.9, map_size=4):\n",
        "    \"\"\"\n",
        "    Asynchronous Value Iteration Algorithm.\n",
        "\n",
        "    Args:\n",
        "        env: gym environment\n",
        "        theta: float, stopping criteria\n",
        "        discount_factor: float, gamma discount factor\n",
        "        map_size: int, FrozenLake map size\n",
        "    Returns:\n",
        "        tuple of (policy, V)\n",
        "        policy: numpy array with size [s,a], each element represents probability of action given state\n",
        "        V: numpy array with size [s,], value table\n",
        "\n",
        "    (IMPORTANT!): For each iteration, use visualize_value_table for visualization!\n",
        "    \"\"\"\n",
        "    policy, V = None, None\n",
        "    ######## TODO ########\n",
        "    V = np.zeros(env.observation_space.n)\n",
        "    iterations = 0\n",
        "    while True:\n",
        "      delta = 0\n",
        "      iterations += 1\n",
        "      for state in reversed(range(env.observation_space.n)):\n",
        "        action_values = one_step_lookahead(state, V, env, discount_factor)\n",
        "        max_value = np.max(action_values)\n",
        "\n",
        "        delta = max(delta, abs(max_value - V[state]))\n",
        "        V[state] = max_value\n",
        "\n",
        "      visualize_value_table(V, map_size)\n",
        "\n",
        "      if delta < theta:\n",
        "          break\n",
        "\n",
        "    policy = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "    for state in range(env.observation_space.n):\n",
        "      action_values = one_step_lookahead(state, V, env, discount_factor)\n",
        "      best_action = np.argmax(action_values)\n",
        "      policy[state, best_action] = 1.0\n",
        "    print(\"Number of iterations\",iterations)\n",
        "    ######################\n",
        "    return policy, V"
      ],
      "metadata": {
        "id": "T8fahrb3edVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "policy, v = asynchronous_value_iteration(env)\n",
        "visualize_greedy_policy(policy, map_size)"
      ],
      "metadata": {
        "id": "utc8D3Ba_rNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<mark>TODO-11</mark> Answer the following questions.\n",
        "\n",
        "*   From the results from value iteration and asynchronous value iteration, you can see that convergence speeds (# of iterations) are same. Discuss why this happens and devise how to make asynchoronous value iteration converges faster than value iteration (hint: reverse the loop over state).\n"
      ],
      "metadata": {
        "id": "U-3AiZpDoew8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<mark>Answer for TODO-11</mark>\n",
        "* I found that my asynchronous value iteration converged at the same speed as the standard version because I was still iterating through the states in a forward order. This meant the reward information from the goal state only propagated to other states in the next full sweep, which negated the benefit of the in-place updates. However, when I reversed the order of iteration, it converged faster than the standard version."
      ],
      "metadata": {
        "id": "ocjhCd6gcyV9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. More Investigations on FrozenLake"
      ],
      "metadata": {
        "id": "ifXuZW5lnwxo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to explore more on `FrozenLake` environment. We now consider the case `is_slippery=True`, where the agent will move in intended direction with probability of $1/3$ else will move in either perpendicular direction with equal probability of $1/3$ in both directions.\n",
        "\n",
        "For example, if action is GO LEFT and `is_slippery=True`, then:\n",
        "- $P(\\text{move left})=1/3$\n",
        "- $P(\\text{move up})=1/3$\n",
        "- $P(\\text{move down})=1/3$\n",
        "\n",
        "Now, let's make new environment and do policy iteration!"
      ],
      "metadata": {
        "id": "wyNS7_D7thb8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "map_size = 4\n",
        "is_slippery = True\n",
        "env = gym.make(\"FrozenLake-v1\", desc=None, map_name=f\"{map_size}x{map_size}\", is_slippery=is_slippery).unwrapped"
      ],
      "metadata": {
        "id": "JWPxHhOsrsE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "policy, v = policy_iteration(env, go_left_policy(env))"
      ],
      "metadata": {
        "id": "E9hscF56rtz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<mark>TODO-12</mark> Answer the following questions.\n",
        "\n",
        "*   Explain the difference between optimal policy in `slippery FrozenLake` and `FrozenLake`."
      ],
      "metadata": {
        "id": "oDx_8Yvvtcqm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<mark>Answer for TODO-12</mark>\n",
        "* **Non-Slippery (Predictable)**: The optimal policy is the shortest path to the goal. Since movement is certain, the agent can take the most direct route without danger. **Slippery (Unpredictable)**: The optimal policy is a risk-averse strategy. Because of the chance of slipping into a hole, the agent learns to take longer, safer paths that stay away from hazards, prioritizing survival over speed."
      ],
      "metadata": {
        "id": "yUdubANMc2FB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "47vlKIVFek4X"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Rl4EvnTN8fQE",
        "nHkyvNOhp-zM"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}